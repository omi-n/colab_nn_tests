{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN/LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNH9CxTfFNwGc/8SxFltFk/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO1_XpWhrXZi"
      },
      "source": [
        "## Checking GPU, import libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8414WcZzinN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53087671-4f8c-43ed-e82b-f9c159186beb"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul  7 00:07:26 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ovO-JIJrS3q"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_Zb7qIErbK6"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIdXycwr0p1D",
        "outputId": "433eac46-d259-4fd4-baaa-1cfda24c0e88"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_4lQ4Ma1FrQ",
        "outputId": "1d03b7bd-2be9-4c0a-d905-bccbc3caf106"
      },
      "source": [
        "!ls gdrive/MyDrive/MachineLearning/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " anna.txt  'Necron-bridge(2).txt'   rnn.net\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoLyNFTkrdpk"
      },
      "source": [
        " with open('gdrive/MyDrive/MachineLearning/pride-prej.txt', 'r') as f:\n",
        "   text = f.read()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SZ20A-D3GRk"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "  # Initialize\n",
        "  one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "  # Fill with zero\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "  # Reshape to original\n",
        "  one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "  return one_hot"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lYeI0G1dxgD"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "  total_batch_size = batch_size * seq_length\n",
        "\n",
        "  # find the amount of batches we can make in the array\n",
        "  n_batches = len(arr) // total_batch_size\n",
        "\n",
        "  # following will cut out anything in the array we're not going to use\n",
        "  arr = arr[:n_batches * total_batch_size]\n",
        "\n",
        "  # reshape into rows to separate batches\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "    x = arr[:, n:n+seq_length]\n",
        "\n",
        "    y = np.zeros_like(x)\n",
        "    try:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "    except IndexError:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "    yield x, y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8NB1geNsqK4"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvoGtBB5gXkj"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "  def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.3, lr=3E-4):\n",
        "    super(CharRNN, self).__init__()\n",
        "\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "\n",
        "    self.chars = tokens\n",
        "    self.int_char = dict(enumerate(self.chars))\n",
        "    self.char_int = {ch: ii for ii, ch in self.int_char.items()}\n",
        "\n",
        "    # model layers\n",
        "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob,\n",
        "                        batch_first=True)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "    # apply dropout, reshape, shove to fully connected\n",
        "    out = self.dropout(r_output)\n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    # return output, hidden state\n",
        "    return out, hidden\n",
        "\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "              weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "    \n",
        "    return hidden"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpaFZfh5srhT"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgevdHFDizhp"
      },
      "source": [
        "def train_loop(model, data, epochs=10, batch_size=10, seq_length=50, lr=3E-4, clip=5, valid_frac=0.2, print_every=100):\n",
        "  model.to(device)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "  valid_index = int(len(data) * (1 - valid_frac))\n",
        "  data, valid_data = data[:valid_index], data[valid_index:]\n",
        "\n",
        "  count = 0\n",
        "  count_since_last_save = 0\n",
        "  n_chars = len(model.chars)\n",
        "  valid_loss_min = np.Inf\n",
        "  model.train()\n",
        "\n",
        "  for e in range(epochs):\n",
        "    h = model.init_hidden(batch_size)\n",
        "\n",
        "    for x, y in get_batches(data, batch_size, seq_length):\n",
        "      count += 1 # python please ++\n",
        "      x = one_hot_encode(x, n_chars)\n",
        "      inputs, targets = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
        "\n",
        "      h = tuple([each.data for each in h])\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      with torch.cuda.amp.autocast():\n",
        "        output, h = model(inputs, h)\n",
        "      \n",
        "      loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "      loss.backward()\n",
        "\n",
        "      nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "      optimizer.step()\n",
        "\n",
        "      if count % print_every == 0:\n",
        "        valid_h = model.init_hidden(batch_size)\n",
        "        valid_losses = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          for x, y in get_batches(valid_data, batch_size, seq_length):\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "              x, y = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
        "\n",
        "            valid_h = tuple([each.data for each in valid_h])\n",
        "\n",
        "            inputs, targets = x, y\n",
        "            \n",
        "            output, valid_h = model(inputs, valid_h)\n",
        "            valid_loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "\n",
        "            valid_losses.append(valid_loss.item())\n",
        "        \n",
        "        count_since_last_save += print_every\n",
        "        valid_loss_mean = np.mean(valid_losses)\n",
        "        model.train()\n",
        "\n",
        "        print( \"-------------------------------------\\n\"\n",
        "              f\"Epoch: {e + 1} / {epochs}\\n\"\n",
        "              f\"Step: {count}\\n\"\n",
        "              f\"Training Loss: {loss.item(): .4f}\\n\"\n",
        "              f\"Validation Loss: {valid_loss_mean: .4f}\\n\")\n",
        "        \n",
        "        if valid_loss_mean <= valid_loss_min:\n",
        "          valid_loss_min = valid_loss_mean\n",
        "          print(\"Validation Loss Decreased! Saving Model Params.\")\n",
        "          model_name = 'rnn.net'\n",
        "          checkpoint = {'n_hidden': model.n_hidden,\n",
        "                        'n_layers': model.n_layers,\n",
        "                        'state_dict': model.state_dict(),\n",
        "                        'tokens': model.chars}\n",
        "          with open(model_name, 'wb') as f:\n",
        "            torch.save(checkpoint, f)\n",
        "          count_since_last_save = 0\n",
        "\n",
        "        print(f\"Steps Since Last Save: {count_since_last_save} / {print_every * 7}\")\n",
        "\n",
        "    if count_since_last_save >= print_every * 7:\n",
        "      print(\"Stopping training. Validation Loss has stopped decreasing.\")\n",
        "      break  "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNbcDRVimuq_",
        "outputId": "32b9837e-5ae0-46db-8828-e5559a10280f"
      },
      "source": [
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "chars = tuple(set(text))\n",
        "int_char = dict(enumerate(chars))\n",
        "char_int = {ch: ii for ii, ch in int_char.items()}\n",
        "encoded = np.array([char_int[ch] for ch in text])\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(92, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=92, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac0BMpoonAJ_",
        "outputId": "f63b3eb1-cb59-4de2-df55-759980c28f18"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 200\n",
        "\n",
        "# train the model\n",
        "train_loop(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=3E-4, print_every=30)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "Epoch: 1 / 200\n",
            "Step: 30\n",
            "Training Loss:  1.0674\n",
            "Validation Loss:  1.2858\n",
            "\n",
            "Validation Loss Decreased! Saving Model Params.\n",
            "Steps Since Last Save: 0 / 210\n",
            "-------------------------------------\n",
            "Epoch: 2 / 200\n",
            "Step: 60\n",
            "Training Loss:  1.0576\n",
            "Validation Loss:  1.2868\n",
            "\n",
            "Steps Since Last Save: 30 / 210\n",
            "-------------------------------------\n",
            "Epoch: 2 / 200\n",
            "Step: 90\n",
            "Training Loss:  1.0752\n",
            "Validation Loss:  1.2901\n",
            "\n",
            "Steps Since Last Save: 60 / 210\n",
            "-------------------------------------\n",
            "Epoch: 3 / 200\n",
            "Step: 120\n",
            "Training Loss:  1.0488\n",
            "Validation Loss:  1.2809\n",
            "\n",
            "Validation Loss Decreased! Saving Model Params.\n",
            "Steps Since Last Save: 0 / 210\n",
            "-------------------------------------\n",
            "Epoch: 4 / 200\n",
            "Step: 150\n",
            "Training Loss:  1.0938\n",
            "Validation Loss:  1.2865\n",
            "\n",
            "Steps Since Last Save: 30 / 210\n",
            "-------------------------------------\n",
            "Epoch: 4 / 200\n",
            "Step: 180\n",
            "Training Loss:  1.0801\n",
            "Validation Loss:  1.2756\n",
            "\n",
            "Validation Loss Decreased! Saving Model Params.\n",
            "Steps Since Last Save: 0 / 210\n",
            "-------------------------------------\n",
            "Epoch: 5 / 200\n",
            "Step: 210\n",
            "Training Loss:  1.0273\n",
            "Validation Loss:  1.2919\n",
            "\n",
            "Steps Since Last Save: 30 / 210\n",
            "-------------------------------------\n",
            "Epoch: 5 / 200\n",
            "Step: 240\n",
            "Training Loss:  1.1191\n",
            "Validation Loss:  1.2973\n",
            "\n",
            "Steps Since Last Save: 60 / 210\n",
            "-------------------------------------\n",
            "Epoch: 6 / 200\n",
            "Step: 270\n",
            "Training Loss:  1.0449\n",
            "Validation Loss:  1.2767\n",
            "\n",
            "Steps Since Last Save: 90 / 210\n",
            "-------------------------------------\n",
            "Epoch: 7 / 200\n",
            "Step: 300\n",
            "Training Loss:  1.0361\n",
            "Validation Loss:  1.2889\n",
            "\n",
            "Steps Since Last Save: 120 / 210\n",
            "-------------------------------------\n",
            "Epoch: 7 / 200\n",
            "Step: 330\n",
            "Training Loss:  1.0566\n",
            "Validation Loss:  1.2844\n",
            "\n",
            "Steps Since Last Save: 150 / 210\n",
            "-------------------------------------\n",
            "Epoch: 8 / 200\n",
            "Step: 360\n",
            "Training Loss:  1.0254\n",
            "Validation Loss:  1.2748\n",
            "\n",
            "Validation Loss Decreased! Saving Model Params.\n",
            "Steps Since Last Save: 0 / 210\n",
            "-------------------------------------\n",
            "Epoch: 9 / 200\n",
            "Step: 390\n",
            "Training Loss:  1.0752\n",
            "Validation Loss:  1.2856\n",
            "\n",
            "Steps Since Last Save: 30 / 210\n",
            "-------------------------------------\n",
            "Epoch: 9 / 200\n",
            "Step: 420\n",
            "Training Loss:  1.0547\n",
            "Validation Loss:  1.2790\n",
            "\n",
            "Steps Since Last Save: 60 / 210\n",
            "-------------------------------------\n",
            "Epoch: 10 / 200\n",
            "Step: 450\n",
            "Training Loss:  1.0166\n",
            "Validation Loss:  1.2780\n",
            "\n",
            "Steps Since Last Save: 90 / 210\n",
            "-------------------------------------\n",
            "Epoch: 10 / 200\n",
            "Step: 480\n",
            "Training Loss:  1.1025\n",
            "Validation Loss:  1.2767\n",
            "\n",
            "Steps Since Last Save: 120 / 210\n",
            "-------------------------------------\n",
            "Epoch: 11 / 200\n",
            "Step: 510\n",
            "Training Loss:  1.0273\n",
            "Validation Loss:  1.2760\n",
            "\n",
            "Steps Since Last Save: 150 / 210\n",
            "-------------------------------------\n",
            "Epoch: 12 / 200\n",
            "Step: 540\n",
            "Training Loss:  1.0283\n",
            "Validation Loss:  1.2744\n",
            "\n",
            "Validation Loss Decreased! Saving Model Params.\n",
            "Steps Since Last Save: 0 / 210\n",
            "-------------------------------------\n",
            "Epoch: 12 / 200\n",
            "Step: 570\n",
            "Training Loss:  1.0312\n",
            "Validation Loss:  1.2717\n",
            "\n",
            "Validation Loss Decreased! Saving Model Params.\n",
            "Steps Since Last Save: 0 / 210\n",
            "-------------------------------------\n",
            "Epoch: 13 / 200\n",
            "Step: 600\n",
            "Training Loss:  1.0117\n",
            "Validation Loss:  1.2819\n",
            "\n",
            "Steps Since Last Save: 30 / 210\n",
            "-------------------------------------\n",
            "Epoch: 14 / 200\n",
            "Step: 630\n",
            "Training Loss:  1.0518\n",
            "Validation Loss:  1.2757\n",
            "\n",
            "Steps Since Last Save: 60 / 210\n",
            "-------------------------------------\n",
            "Epoch: 14 / 200\n",
            "Step: 660\n",
            "Training Loss:  1.0400\n",
            "Validation Loss:  1.2728\n",
            "\n",
            "Steps Since Last Save: 90 / 210\n",
            "-------------------------------------\n",
            "Epoch: 15 / 200\n",
            "Step: 690\n",
            "Training Loss:  1.0010\n",
            "Validation Loss:  1.2782\n",
            "\n",
            "Steps Since Last Save: 120 / 210\n",
            "-------------------------------------\n",
            "Epoch: 15 / 200\n",
            "Step: 720\n",
            "Training Loss:  1.0811\n",
            "Validation Loss:  1.2794\n",
            "\n",
            "Steps Since Last Save: 150 / 210\n",
            "-------------------------------------\n",
            "Epoch: 16 / 200\n",
            "Step: 750\n",
            "Training Loss:  1.0078\n",
            "Validation Loss:  1.2715\n",
            "\n",
            "Validation Loss Decreased! Saving Model Params.\n",
            "Steps Since Last Save: 0 / 210\n",
            "-------------------------------------\n",
            "Epoch: 17 / 200\n",
            "Step: 780\n",
            "Training Loss:  1.0068\n",
            "Validation Loss:  1.3029\n",
            "\n",
            "Steps Since Last Save: 30 / 210\n",
            "-------------------------------------\n",
            "Epoch: 17 / 200\n",
            "Step: 810\n",
            "Training Loss:  1.0166\n",
            "Validation Loss:  1.2936\n",
            "\n",
            "Steps Since Last Save: 60 / 210\n",
            "-------------------------------------\n",
            "Epoch: 18 / 200\n",
            "Step: 840\n",
            "Training Loss:  1.0010\n",
            "Validation Loss:  1.2676\n",
            "\n",
            "Validation Loss Decreased! Saving Model Params.\n",
            "Steps Since Last Save: 0 / 210\n",
            "-------------------------------------\n",
            "Epoch: 19 / 200\n",
            "Step: 870\n",
            "Training Loss:  1.0322\n",
            "Validation Loss:  1.2922\n",
            "\n",
            "Steps Since Last Save: 30 / 210\n",
            "-------------------------------------\n",
            "Epoch: 19 / 200\n",
            "Step: 900\n",
            "Training Loss:  1.0332\n",
            "Validation Loss:  1.2864\n",
            "\n",
            "Steps Since Last Save: 60 / 210\n",
            "-------------------------------------\n",
            "Epoch: 20 / 200\n",
            "Step: 930\n",
            "Training Loss:  0.9976\n",
            "Validation Loss:  1.2707\n",
            "\n",
            "Steps Since Last Save: 90 / 210\n",
            "-------------------------------------\n",
            "Epoch: 20 / 200\n",
            "Step: 960\n",
            "Training Loss:  1.0654\n",
            "Validation Loss:  1.2696\n",
            "\n",
            "Steps Since Last Save: 120 / 210\n",
            "-------------------------------------\n",
            "Epoch: 21 / 200\n",
            "Step: 990\n",
            "Training Loss:  0.9937\n",
            "Validation Loss:  1.3030\n",
            "\n",
            "Steps Since Last Save: 150 / 210\n",
            "-------------------------------------\n",
            "Epoch: 22 / 200\n",
            "Step: 1020\n",
            "Training Loss:  0.9937\n",
            "Validation Loss:  1.2788\n",
            "\n",
            "Steps Since Last Save: 180 / 210\n",
            "-------------------------------------\n",
            "Epoch: 22 / 200\n",
            "Step: 1050\n",
            "Training Loss:  0.9995\n",
            "Validation Loss:  1.2696\n",
            "\n",
            "Steps Since Last Save: 210 / 210\n",
            "Stopping training. Validation Loss has stopped decreasing.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzu93AzdstTC"
      },
      "source": [
        "## Test, Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRUAWVkYtZaL",
        "outputId": "ed19740b-5b82-494e-a64a-ff57db5be934"
      },
      "source": [
        "# load saved checkpoint\n",
        "with open('rnn.net', 'rb') as f:\n",
        "  checkpoint = torch.load(f)\n",
        "\n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSLh4uQUFeLr"
      },
      "source": [
        "# copy over the rnn to the machine learning directory\n",
        "!cp rnn.net gdrive/MyDrive/MachineLearning/"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XacxDimq4W-G"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "  # tensor inputs\n",
        "  x = np.array([[net.char_int[char]]])\n",
        "  x = one_hot_encode(x, len(net.chars))\n",
        "  inputs = torch.from_numpy(x).to(device)\n",
        "        \n",
        "  # get the hidden state from the history\n",
        "  h = tuple([each.data for each in h])\n",
        "  # get the output of the model\n",
        "  out, h = net(inputs, h)\n",
        "\n",
        "  # get the character probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "  if train_on_gpu:\n",
        "    p = p.cpu()\n",
        "        \n",
        "  # get top possible characters\n",
        "  if top_k is None:\n",
        "    top_ch = np.arange(len(net.chars))\n",
        "  else:\n",
        "    p, top_ch = p.topk(top_k)\n",
        "    top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "  # select the likely next character with some element of randomness\n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "  # return the value of the character, hidden state (encoded)\n",
        "  return net.int_char[char], h"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdvPhJrs9Gth"
      },
      "source": [
        "def sample(net, size, prime=\"The\", top_k=None):\n",
        "  net.to(device)\n",
        "\n",
        "  net.eval()\n",
        "\n",
        "  chars = [ch for ch in prime]\n",
        "  h = net.init_hidden(1)\n",
        "  for ch in prime:\n",
        "    char, h = predict(net, ch, h, top_k=top_k)\n",
        "  \n",
        "  chars.append(char)\n",
        "\n",
        "  for ii in range(size):\n",
        "    char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "    chars.append(char)\n",
        "  \n",
        "  return \"\".join(chars)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-2GRyxRCTUE",
        "outputId": "975bca81-2970-4512-900f-81625a647d7f"
      },
      "source": [
        "print(sample(net, 3000, prime='The ', top_k=5))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The asserting to\n",
            "      Mr. Bingley, who assured him that its spilit of him to hear. It\n",
            "      have been to suppose them.”\n",
            "\n",
            "      “We more concerned in the word which the carriage will be more\n",
            "      to believe her for ever as my farcy, there are money of father.”\n",
            "\n",
            "      Elizabeth was so such a point of such compliments, before the\n",
            "      attachment; and then were sometimes together were a longer to ten\n",
            "      this; and then at the montion, that he had the call of such many, and\n",
            "      at the manner, they hoped their advice, and which was not to\n",
            "      compliment them, the little concern of which he could not\n",
            "      be disposed to see me for the words at all that he was too much.\n",
            "\n",
            "      Mrs. Bennet should have a stronger occepled the done; and she had\n",
            "      recollected the chacked both, at her family, the probable and\n",
            "      concern that he was not such a stranger would have been sisterly\n",
            "      thoughted at home, and satisfied him with his serses and\n",
            "      particulars of seeing a ladies and such a simplesent and so fellowing\n",
            "      of him as to this subject, was still a gentle man excused a wert\n",
            "      always chooch of all hertent, and then to be disagreedent to hear\n",
            "      their acceptanish there whom he was soon answered by home, after her\n",
            "      acknowledged of the reated which made a stranger would she had, and\n",
            "      said: “I have no most folteration may be off.”\n",
            "\n",
            "      “I am sire shall,” replied her attention was conversation; and the\n",
            "      compliment to their sister, so was astonidily towards Miss Bingley.\n",
            "\n",
            "      “You are not design in the whole daughters.”\n",
            "\n",
            "      “I am now considered of him only sensible,” cried Elizabeth.\n",
            "\n",
            "      “Your condecting this is not one soon. How alliet that he is\n",
            "      to bank to myself it included to her. I know nothing will of a\n",
            "      more compassion. The father who chooses, had been to me she was is\n",
            "      able to be deceased in the seen. I ampressed in that more than\n",
            "      your own founting any day with the girl of a moment to be\n",
            "      conversation; for, I assured the times, he is now some time that\n",
            "      you are suffered from making any of her manners. I suppose I shall\n",
            "      be the matter. I am sorry of handsome attempt to his mention as\n",
            "      anybody will be an activation! It, when as he could takly by\n",
            "      the subjects, of his father was no occasitated as all. Their\n",
            "      countenance is now there we are not to tell him with her, and if\n",
            "      you will never be so much to make at all the others. What as you\n",
            "      who could have not so such a perfect call. But why has been in the\n",
            "      most better is an anywing to be such mind, was not a minute, but\n",
            "      she caneflected them. She had a few words are all, and what his\n",
            "      disposition all the subject of the whole and mance after a love\n",
            "      in merelow, and if I consider the face on the object of him\n",
            "      in the cousin’s being in the sumper and poust after three surs\n",
            "      of his character, we are all this. I do not know to be so much\n",
            "      to be seen, which it as he well i\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}